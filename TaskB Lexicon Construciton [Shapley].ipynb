{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2255,"status":"ok","timestamp":1675615231259,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"},"user_tz":0},"id":"sHyS00BLlWd6","outputId":"2dfb0482-4228-4a39-bd34-bb6ff5a934e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1675615231259,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"},"user_tz":0},"id":"vd0eo8cegRT8","outputId":"d70b879b-0e89-454d-e195-c6c8e8941845"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/.shortcut-targets-by-id/1lC-ZKLaCDQyfLcof2Ak7FDa6IvTt318A/SemEval2023/SemEval2022-Task10/re_run\n"]}],"source":["cd /content/gdrive/MyDrive/SemEval2023/SemEval2022-Task10/re_run"]},{"cell_type":"code","source":[],"metadata":{"id":"-40XIl5Hz8zy","executionInfo":{"status":"ok","timestamp":1675615231260,"user_tz":0,"elapsed":6,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Mcg-u_-QghEO","executionInfo":{"status":"ok","timestamp":1675615231962,"user_tz":0,"elapsed":708,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","source":[],"metadata":{"id":"DpYnTvGWRidT","executionInfo":{"status":"ok","timestamp":1675615231962,"user_tz":0,"elapsed":3,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"KxVljKIpjonw","executionInfo":{"status":"ok","timestamp":1675615232765,"user_tz":0,"elapsed":805,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","target_column = \"label_category\"\n","data = pd.read_csv(\"../Data/starting_ki/train_all_tasks.csv\")\n","\n","# target_column = \"label_vector\"\n","# data = pd.read_csv(\"../GeneratedTexts/task_c_generated_text.csv\")\n","# data[target_column] = data[\"label\"]\n","\n","data = data[data[target_column]!=\"none\"]\n","data.reset_index(inplace=True)\n","data.drop(columns=['index'], inplace=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"7MroRLHpw9xL","executionInfo":{"status":"ok","timestamp":1675615232765,"user_tz":0,"elapsed":8,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"g6Hq_qExqg3g","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1675615232765,"user_tz":0,"elapsed":8,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}},"outputId":"5072ab39-412f-4200-bc67-c09737aebbed"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                          rewire_id  text  label_sexist  \\\n","label_category                                                            \n","1. threats, plans to harm and incitement        310   310           310   \n","2. derogation                                  1590  1590          1590   \n","3. animosity                                   1165  1165          1165   \n","4. prejudiced discussions                       333   333           333   \n","\n","                                          label_vector  \n","label_category                                          \n","1. threats, plans to harm and incitement           310  \n","2. derogation                                     1590  \n","3. animosity                                      1165  \n","4. prejudiced discussions                          333  "],"text/html":["\n","  <div id=\"df-51fc5425-074e-42f3-b33a-be6dbaa6f34a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>rewire_id</th>\n","      <th>text</th>\n","      <th>label_sexist</th>\n","      <th>label_vector</th>\n","    </tr>\n","    <tr>\n","      <th>label_category</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1. threats, plans to harm and incitement</th>\n","      <td>310</td>\n","      <td>310</td>\n","      <td>310</td>\n","      <td>310</td>\n","    </tr>\n","    <tr>\n","      <th>2. derogation</th>\n","      <td>1590</td>\n","      <td>1590</td>\n","      <td>1590</td>\n","      <td>1590</td>\n","    </tr>\n","    <tr>\n","      <th>3. animosity</th>\n","      <td>1165</td>\n","      <td>1165</td>\n","      <td>1165</td>\n","      <td>1165</td>\n","    </tr>\n","    <tr>\n","      <th>4. prejudiced discussions</th>\n","      <td>333</td>\n","      <td>333</td>\n","      <td>333</td>\n","      <td>333</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51fc5425-074e-42f3-b33a-be6dbaa6f34a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-51fc5425-074e-42f3-b33a-be6dbaa6f34a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-51fc5425-074e-42f3-b33a-be6dbaa6f34a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}],"source":["data.groupby(target_column).count()"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"KMhjlSKfsLbV","executionInfo":{"status":"ok","timestamp":1675615232766,"user_tz":0,"elapsed":7,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"outputs":[],"source":["if target_column == \"label_category\":\n","  label_values = [\n","      '1. threats, plans to harm and incitement',\n","      '2. derogation',\n","      '3. animosity',\n","      '4. prejudiced discussions',\n","  ]\n","\n","elif target_column == \"label_vector\":\n","  label_values = [\n","      '1.1 threats of harm',\n","      '1.2 incitement and encouragement of harm',\n","      '2.1 descriptive attacks',\n","      '2.2 aggressive and emotive attacks',\n","      '2.3 dehumanising attacks & overt sexual objectification',\n","      '3.1 casual use of gendered slurs, profanities, and insults',\n","      '3.2 immutable gender differences and gender stereotypes',\n","      '3.3 backhanded gendered compliments',\n","      '3.4 condescending explanations or unwelcome advice',\n","      '4.1 supporting mistreatment of individual women',\n","      '4.2 supporting systemic discrimination against women as a group',\n","  ]"]},{"cell_type":"markdown","metadata":{"id":"S4LEXa0LTwbj"},"source":["## Calculate Shapley"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":11007,"status":"ok","timestamp":1675615245573,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"},"user_tz":0},"id":"hNKX-daoT0sc"},"outputs":[],"source":["!pip install -q transformers shap"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":1232,"status":"ok","timestamp":1675615487143,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"},"user_tz":0},"id":"QiUYmNhwT4IQ"},"outputs":[],"source":["from transformers import BertTokenizer\n","swtokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]},{"cell_type":"code","source":["ls Models"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Va9tL3FRNrHd","executionInfo":{"status":"ok","timestamp":1675615249271,"user_tz":0,"elapsed":20,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}},"outputId":"b04c0221-0533-4b24-b065-5bcb168c542f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mfine-tuned-bert\u001b[0m/  \u001b[01;34mfine-tuned-bertweet\u001b[0m/  \u001b[01;34mfine-tuned-twhinbert\u001b[0m/\n"]}]},{"cell_type":"code","execution_count":10,"metadata":{"id":"SBrnlO3pUDgj","executionInfo":{"status":"ok","timestamp":1675615253554,"user_tz":0,"elapsed":4291,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"outputs":[],"source":["import torch\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","out_dir = f'Models/fine-tuned-bert'\n","model = BertForSequenceClassification.from_pretrained(out_dir)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"bDGUbw-CTlYf","executionInfo":{"status":"ok","timestamp":1675615253555,"user_tz":0,"elapsed":11,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"outputs":[],"source":["import transformers\n","import torch\n","import numpy as np\n","import scipy as sp"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"D6L0tWLQUZoH","executionInfo":{"status":"ok","timestamp":1675615253555,"user_tz":0,"elapsed":9,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"outputs":[],"source":["texts = data['text'].values"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"5v03chzpTlbH","executionInfo":{"status":"ok","timestamp":1675615253969,"user_tz":0,"elapsed":423,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"outputs":[],"source":["import shap\n","# define a prediction function\n","def f(texts):\n","  text_ids = [tokenizer.encode(text, max_length=100, padding='max_length', truncation=True) for text in texts]\n","\n","  att_masks = []\n","  for ids in text_ids:\n","      masks = [int(id > 0) for id in ids]\n","      att_masks.append(masks)\n","\n","  text_ids = torch.tensor(text_ids).to(device)\n","  att_masks = torch.tensor(att_masks).to(device)\n","\n","  outputs = model(text_ids, attention_mask=att_masks)\n","  outputs = outputs[0].detach().cpu().numpy()\n","  scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n","  \n","  val = sp.special.logit(scores[:,1]) # use one vs rest logit units\n","  return val"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"xM6Vf6wWYDSq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675615253970,"user_tz":0,"elapsed":10,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}},"outputId":"7b9b8de9-37d4-4ae4-c803-6a33f8908655"},"outputs":[{"output_type":"stream","name":"stdout","text":["1. threats, plans to harm and incitement\n","2. derogation\n","3. animosity\n","4. prejudiced discussions\n"]}],"source":["for label in label_values:\n","  print(label)"]},{"cell_type":"code","source":[],"metadata":{"id":"EtFUmTeFshPX","executionInfo":{"status":"ok","timestamp":1675615253970,"user_tz":0,"elapsed":4,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"id":"1oZDhfaeYZt2","executionInfo":{"status":"ok","timestamp":1675615253970,"user_tz":0,"elapsed":4,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"outputs":[],"source":["# import pickle\n","\n","# def save_shap_values(filepath, obj):\n","#   with open(filepath, 'wb') as fin:\n","#     pickle.dump(obj, fin)\n","    \n","\n","# explainer = shap.Explainer(f, tokenizer)\n","\n","# d = data\n","# d = {\"text\": d[\"text\"].values}\n","# shap_values = explainer(d, fixed_context=1, batch_size=256)\n","# save_shap_values(f\"./Results/shapley_values.pickle\", shap_values)"]},{"cell_type":"code","source":["import pickle\n","\n","def load_shap_values(filepath):\n","  with open(filepath, 'rb') as fin:\n","    obj = pickle.load(fin)\n","  return obj\n","\n","shap_values = load_shap_values(f\"./Results/shapley_values.pickle\")"],"metadata":{"id":"580ZvNNJPeX_","executionInfo":{"status":"ok","timestamp":1675615331089,"user_tz":0,"elapsed":366,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uDYNYsa2zPei","executionInfo":{"status":"ok","timestamp":1675615253970,"user_tz":0,"elapsed":4,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def get_lexicons(train, shap_values):\n","  lexicons = {}\n","  for idx, label in enumerate(label_values):\n","    d = train[train[target_column]==label]\n","    d = data.reset_index().merge(d[[\"rewire_id\"]], on=\"rewire_id\").set_index('index')\n","    s = shap_values.abs[d.index.to_numpy()]\n","\n","    feature_names = s.mean(0).feature_names\n","    shapley_values = s.mean(0).values\n","\n","    sorted_values = sorted(zip(shapley_values, feature_names), key=lambda pair: -pair[0])\n","    lexicons[label] = {x:v for v, x in sorted_values}\n","  return lexicons\n","  \n","# train = pd.read_csv(f\"Data/0_train.csv\")\n","# lexicons = get_lexicons(train, shap_values)"],"metadata":{"id":"p4ZeLMbaW8fg","executionInfo":{"status":"ok","timestamp":1675615253971,"user_tz":0,"elapsed":5,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PnnRpGI-W8iV","executionInfo":{"status":"ok","timestamp":1675615298576,"user_tz":0,"elapsed":313,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# for k in lexicons:\n","#   for sw in lexicons[k]:\n","#     if sw.startswith(\"#\") or sw.startswith(\"_\"):\n","#       print(sw)\n","#     if sw.endswith(\"#\") or sw.endswith(\"_\"):\n","#       print(sw)"],"metadata":{"id":"BuI8hsHTW8lM","executionInfo":{"status":"ok","timestamp":1675615299616,"user_tz":0,"elapsed":2,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","execution_count":25,"metadata":{"id":"ZYJsKprEtwf_","executionInfo":{"status":"ok","timestamp":1675615300199,"user_tz":0,"elapsed":2,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# Handle subwords"],"metadata":{"id":"jTxTc7NJyp_f"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"id":"JLzzyV46yszN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675615300964,"user_tz":0,"elapsed":441,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}},"outputId":"8b61651d-2e8e-45f0-d8ae-60a98f674685"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["from nltk.tokenize import TweetTokenizer\n","nltktokenizer = TweetTokenizer()\n","\n","def word_tokenize(sent):\n","  # return nltk.word_tokenize(sent)\n","  return nltktokenizer.tokenize(sent)"],"metadata":{"id":"SYFCwpHZzMhK","executionInfo":{"status":"ok","timestamp":1675615300964,"user_tz":0,"elapsed":6,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fYRqFHXCyrSZ","executionInfo":{"status":"ok","timestamp":1675615559173,"user_tz":0,"elapsed":4,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["from itertools import groupby\n","def clean_and_tokenize(sent):\n","  sent = sent.lower().replace(\"#\", \"\").replace(\"''\", '\"').replace(\".\", \" \").replace(\"-\", \" \").replace(\"&x200b;\", \"\")\n","\n","  # remove more than 3 consecutive repeated characters\n","  groups = groupby(sent)\n","  sent = \"\".join([label*min(3, sum(1 for _ in group)) for label, group in groups])\n","  \n","\n","  sent = sent.encode(\"ascii\", \"ignore\")\n","  sent = sent.decode()\n","  \n","  words = word_tokenize(sent)\n","  words = normalise_quote(words)\n","    \n","  subwords = swtokenizer.tokenize(sent)\n","  subwords = normalise_quote(subwords)\n","  return words, subwords\n","\n","def merge_score(lexicons, idxs, subwords):\n","  new_lexicons = {}\n","  for idx in idxs:\n","    w = \"\"\n","    s = 0\n","    for i in idx:\n","      sw = subwords[i].replace(\"#\", \"\")\n","      w += sw\n","      if sw not in lexicons[label]:\n","      #   print(\">>\", sw)\n","        continue\n","\n","      s += lexicons[label][sw]\n","    if w in new_lexicons:\n","      new_lexicons[w] = max(new_lexicons[w], s)\n","    else:\n","      new_lexicons[w] = s\n","  return new_lexicons\n","  "],"metadata":{"id":"Lync91zldlbz","executionInfo":{"status":"ok","timestamp":1675616319709,"user_tz":0,"elapsed":759,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["from collections import defaultdict\n","\n","def map_subwords(words, subwords):\n","  sidx = 0\n","  widx = 0\n","  w = \"\"\n","  mapping = []\n","\n","  while widx < len(words):\n","    w = w + words[widx]\n","    if sidx >= len(subwords):\n","      print(widx, w, sidx)\n","      print(words)\n","      print(subwords)\n","\n","    idx = [sidx]\n","    s = subwords[sidx].replace(\"#\", \"\")\n","\n","    while len(w) < len(s):\n","      widx += 1\n","      w = w + words[widx].strip()\n","\n","    # if words[0] == \"thotlife\":\n","    #     print(w, s)\n","\n","    while s!=w and len(s) < len(w) and sidx+1 < len(subwords):\n","      sidx += 1\n","      s += subwords[sidx].replace(\"#\", \"\").strip()\n","      idx.append(sidx)\n","\n","      # if words[0] == \"thotlife\":\n","      #   print(f\"[{w}] [{s}]\", len(w), len(s), w==s)\n","\n","    if len(s) > len(w):\n","      widx += 1\n","      sidx -= len(idx) - 1\n","      continue\n","    \n","  \n","    if len(w) > 20 and w not in words:\n","      print(\"ERROR\", w)\n","\n","      # assert(False)    \n","    sidx += 1\n","    # print(w, s, idx)\n","    widx += 1\n","    w = \"\"\n","\n","    mapping.append(idx)\n","\n","  return mapping\n","\n","def normalise_quote(tokens):\n","  for widx, w in enumerate(tokens):\n","    if w==\"``\":\n","      tokens[widx] = '\"'\n","    elif w==\"''\":\n","      tokens[widx] = '\"'\n","    elif w==\"... ...\":\n","      tokens[widx] = '......'\n","      \n","  return tokens\n","\n","\n","from itertools import groupby\n","\n","def merge_subwords(lexicons, train):\n","  vocabs = {}\n","  new_lexicons_scores = {}\n","\n","  for idx, label in enumerate(label_values):\n","    print(\"Working on\", label)\n","    new_lexicons = defaultdict(int)\n","\n","    d = train[train[target_column]==label]\n","    texts = d[\"text\"].values\n","\n","    for sent in texts:\n","      words, subwords = clean_and_tokenize(sent)\n","      idxs = map_subwords(words, subwords)\n","      \n","      for idx in idxs:\n","        w = \"\"\n","        s = 0\n","        for i in idx:\n","          sw = subwords[i].replace(\"#\", \"\")\n","          w += sw\n","          if sw not in lexicons[label]:\n","          #   print(\">>\", sw)\n","            continue\n","\n","          s += lexicons[label][sw]\n","          \n","        new_lexicons[w] = max(new_lexicons[w], s)\n","  \n","    sorted_list = sorted(new_lexicons.items(), key=lambda item: -item[1])\n","    new_lexicons_scores[label] = { k:v for k, v in sorted_list if v > 0}\n","  return new_lexicons_scores\n","\n","\n","# newlexicons = merge_subwords(lexicons, train)\n","# for c in newlexicons:\n","#   newlexicons[c] = {k: v for k, v in sorted(newlexicons[c].items(), key=lambda item: -item[1])}\n","\n","#   print(c, list(newlexicons[c].keys())[0:10])"],"metadata":{"id":"FM1ccXvcdrqN","executionInfo":{"status":"ok","timestamp":1675616492824,"user_tz":0,"elapsed":336,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dbGdpMxuegtR","executionInfo":{"status":"ok","timestamp":1675616498659,"user_tz":0,"elapsed":461,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","def filter_lexicons(lexicons, q=0.8):\n","  threshold = {}\n","  for l in lexicons:\n","    values = [v for k, v in lexicons[l].items()]\n","    threshold[l] = np.quantile(values, q)\n","\n","\n","  new_lexicons = {}\n","  for l in lexicons:\n","    new_lexicons[l] = {}\n","    for w in lexicons[l]:\n","      v = lexicons[l][w]\n","\n","      if v > threshold[l]:\n","        new_lexicons[l][w] = v\n","  return new_lexicons\n","\n","# filtered_lexicons = filter_lexicons(newlexicons, q=0.8)"],"metadata":{"id":"B1cILFs0DvgU","executionInfo":{"status":"ok","timestamp":1675616500822,"user_tz":0,"elapsed":670,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"a4Tz8KeJdrDu","executionInfo":{"status":"ok","timestamp":1675616501868,"user_tz":0,"elapsed":3,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xsVOp8MQAhIP","executionInfo":{"status":"ok","timestamp":1675616502290,"user_tz":0,"elapsed":2,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Mqr2RKSFBJV2","executionInfo":{"status":"ok","timestamp":1675616502290,"user_tz":0,"elapsed":2,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VZJ0vzY9AjqH","executionInfo":{"status":"ok","timestamp":1675616502693,"user_tz":0,"elapsed":9,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bd5SleWYCnz0","executionInfo":{"status":"ok","timestamp":1675616502695,"user_tz":0,"elapsed":10,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-ZSY1v_OC1cf","executionInfo":{"status":"ok","timestamp":1675616502695,"user_tz":0,"elapsed":9,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"GvUkpppJgH5M"}},{"cell_type":"code","source":["from sklearn.metrics import f1_score\n","\n","def predict(word, lexicons):\n","  cnt = {}\n","  for label in label_values:\n","    cnt[label] = 0\n","\n","  for w in word:\n","    for label in label_values:\n","      if w in lexicons[label]:\n","        cnt[label] += lexicons[label][w]\n","\n","  \n","  if sum(cnt.values())==0:\n","    return None\n","  \n","  return max(cnt.items(), key=lambda k: k[1])[0]\n","\n","def run_predict(test_words, test_labels, lexicons, return_predict=False):\n","  y_pred = []\n","  y_test = []\n","  non = 0\n","  for word, label in zip(test_words, test_labels):\n","    pred = predict(word, lexicons)\n","    if pred is None:\n","      non += 1\n","      continue\n","\n","    y_pred.append(pred)\n","    y_test.append(label)\n","\n","  f1 = f1_score(y_test, y_pred, average='macro')\n","  skip = non/len(test_words)\n","\n","  if return_predict:\n","    return f1, skip, y_test, y_pred\n","    \n","  return f1, skip\n","\n","# test = pd.read_csv(f\"Data/0_test.csv\")\n","# test_words = [nltk.word_tokenize(sent) for sent in test[\"text\"].values]\n","# test_labels = test[target_column].values\n","\n","# run_predict(test_words, test_labels, filtered_lexicons)"],"metadata":{"id":"5R1wQL68gMWo","executionInfo":{"status":"ok","timestamp":1675616503877,"user_tz":0,"elapsed":346,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"q3mrxCuLgN8Z","executionInfo":{"status":"ok","timestamp":1675616506018,"user_tz":0,"elapsed":2,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":["## Hypterparameter Tuning"],"metadata":{"id":"bLCzcOcQhHjV"}},{"cell_type":"code","source":["all_f1, all_skip = {}, {}\n","for q in np.arange(0.5, 1, 0.05):\n","  all_f1[q] = []\n","  all_skip[q] = []"],"metadata":{"id":"oJqnWRR6hJX_","executionInfo":{"status":"ok","timestamp":1675616506460,"user_tz":0,"elapsed":4,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"t_afk0l9c_LJ","executionInfo":{"status":"ok","timestamp":1675616508800,"user_tz":0,"elapsed":415,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qy7EeLYOc_Rl","executionInfo":{"status":"ok","timestamp":1675616509238,"user_tz":0,"elapsed":4,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["for i in range(5):\n","  train = pd.read_csv(f\"Data/{i}_train.csv\")\n","\n","  val = pd.read_csv(f\"Data/{i}_val.csv\")\n","  val_words = [word_tokenize(sent) for sent in val[\"text\"].values]\n","  val_labels = val[target_column].values\n","\n","  lexicons = get_lexicons(train, shap_values)\n","  newlexicons = merge_subwords(lexicons, train)\n","  \n","\n","  for q in np.arange(0.5, 1, 0.05):\n","    filtered_lexicons = filter_lexicons(newlexicons, q=q)\n","    f1, skip = run_predict(val_words, val_labels, filtered_lexicons)\n","\n","    all_f1[q].append(f1)\n","    all_skip[q].append(skip)\n","\n","  print(\"DONE\",i)\n","  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"93MSTFr4hOwA","executionInfo":{"status":"ok","timestamp":1675616550375,"user_tz":0,"elapsed":40709,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}},"outputId":"1db0be79-2663-4bc7-b5e4-6d79dbf10294"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stderr","text":["Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]},{"output_type":"stream","name":"stdout","text":["Working on 1. threats, plans to harm and incitement\n","Working on 2. derogation\n","Working on 3. animosity\n","Working on 4. prejudiced discussions\n","DONE 0\n"]},{"output_type":"stream","name":"stderr","text":["Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]},{"output_type":"stream","name":"stdout","text":["Working on 1. threats, plans to harm and incitement\n","Working on 2. derogation\n","Working on 3. animosity\n","Working on 4. prejudiced discussions\n","DONE 1\n"]},{"output_type":"stream","name":"stderr","text":["Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]},{"output_type":"stream","name":"stdout","text":["Working on 1. threats, plans to harm and incitement\n","Working on 2. derogation\n","Working on 3. animosity\n","Working on 4. prejudiced discussions\n","DONE 2\n"]},{"output_type":"stream","name":"stderr","text":["Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]},{"output_type":"stream","name":"stdout","text":["Working on 1. threats, plans to harm and incitement\n","Working on 2. derogation\n","Working on 3. animosity\n","Working on 4. prejudiced discussions\n","DONE 3\n"]},{"output_type":"stream","name":"stderr","text":["Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]},{"output_type":"stream","name":"stdout","text":["Working on 1. threats, plans to harm and incitement\n","Working on 2. derogation\n","Working on 3. animosity\n","Working on 4. prejudiced discussions\n","DONE 4\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","for q in np.arange(0.5, 1, 0.05):\n","  print(f\"{q:.3f} >> F1:{np.mean(all_f1[q]):.3f}±{np.std(all_f1[q]):.3f}, SKIP: {np.mean(all_skip[q]):.3f}±{np.std(all_skip[q]):.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yrZzM_BCh7oQ","executionInfo":{"status":"ok","timestamp":1675616550375,"user_tz":0,"elapsed":27,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}},"outputId":"e8de4a5c-ec2c-4f45-87f1-ab4b3f6743c7"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["0.500 >> F1:0.294±0.019, SKIP: 0.001±0.001\n","0.550 >> F1:0.284±0.028, SKIP: 0.002±0.002\n","0.600 >> F1:0.271±0.021, SKIP: 0.002±0.002\n","0.650 >> F1:0.262±0.012, SKIP: 0.004±0.002\n","0.700 >> F1:0.283±0.015, SKIP: 0.004±0.001\n","0.750 >> F1:0.270±0.029, SKIP: 0.017±0.005\n","0.800 >> F1:0.282±0.022, SKIP: 0.035±0.003\n","0.850 >> F1:0.275±0.031, SKIP: 0.055±0.006\n","0.900 >> F1:0.280±0.044, SKIP: 0.104±0.013\n","0.950 >> F1:0.286±0.035, SKIP: 0.230±0.025\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"6umdIiZKh-67","executionInfo":{"status":"ok","timestamp":1675616550375,"user_tz":0,"elapsed":20,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["for k in lexicons:\n","  print(k, list(lexicons[k])[0:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jUKMBRKrikXb","executionInfo":{"status":"ok","timestamp":1675616550375,"user_tz":0,"elapsed":19,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}},"outputId":"ee253642-a3e2-4b53-f763-d6d80fd4c67e"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["1. threats, plans to harm and incitement ['whore', 'bitch', 'ska', 'pussy', 'feminism', 'ssi', 'feminist', 'prostitutes', 'ars', '3']\n","2. derogation ['isa', 'bitch', 'dyke', 'odle', 'hooker', 'lays', 'pussy', 'ssi', 'whore', 'handles']\n","3. animosity ['bitch', 'pussy', 'auto', 'whore', 'dyke', 'pizza', 'sensible', 'ssi', 'sies', 'cu']\n","4. prejudiced discussions ['whore', 'pussy', 'regretted', 'idiots', 'linda', 'fe', 'ac', 'mothers', 'sub', 'body']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Z7QHAWw2ikax","executionInfo":{"status":"ok","timestamp":1675616571440,"user_tz":0,"elapsed":365,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate Lexicons"],"metadata":{"id":"JWHWkhX9ikil"}},{"cell_type":"code","source":["from sklearn.metrics import precision_recall_fscore_support\n","import json\n","\n","all_f1 = []\n","all_skip = []\n","all_p = []\n","all_r = []\n","\n","for i in range(5):\n","  train = pd.read_csv(f\"Data/{i}_train.csv\")\n","  test = pd.read_csv(f\"Data/{i}_test.csv\")\n","\n","  texts = test[\"text\"].values\n","  test_words = [word_tokenize(sent) for sent in test[\"text\"].values]\n","  test_labels = test[target_column].values\n","\n","  lexicons = get_lexicons(train, shap_values)\n","  newlexicons = merge_subwords(lexicons, train)\n","  filtered_lexicons = filter_lexicons(newlexicons, q=0.90)  \n","\n","  for c in filtered_lexicons:\n","    filtered_lexicons[c] = {k: v for k, v in sorted(filtered_lexicons[c].items(), key=lambda item: -item[1])}\n","  \n","  with open(f'Results/TaskB/lexicon_shapley_train_{i}.json', 'w') as outfile:\n","      json.dump(filtered_lexicons, outfile)\n","\n","  f1, skip, y_test, y_pred = run_predict(test_words, test_labels, filtered_lexicons, return_predict=True)\n","  p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n","\n","  all_p.append(p)\n","  all_r.append(r)\n","  all_f1.append(f1)\n","  all_skip.append(skip)\n","\n","print(f\"F1:{np.mean(all_f1):.3f}±{np.std(all_f1):.3f}, SKIP: {np.mean(all_skip):.3f}±{np.std(all_skip):.3f}\")\n","print(f\"P:{np.mean(all_p):.3f}±{np.std(all_p):.3f}, R: {np.mean(all_r):.3f}±{np.std(all_r):.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DQp2y-1AimJj","executionInfo":{"status":"ok","timestamp":1675616613348,"user_tz":0,"elapsed":41283,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}},"outputId":"cf88646e-05ea-4d97-fd99-1b2cce4abbfa"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stderr","text":["Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]},{"output_type":"stream","name":"stdout","text":["Working on 1. threats, plans to harm and incitement\n","Working on 2. derogation\n","Working on 3. animosity\n","Working on 4. prejudiced discussions\n"]},{"output_type":"stream","name":"stderr","text":["Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]},{"output_type":"stream","name":"stdout","text":["Working on 1. threats, plans to harm and incitement\n","Working on 2. derogation\n","Working on 3. animosity\n","Working on 4. prejudiced discussions\n"]},{"output_type":"stream","name":"stderr","text":["Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]},{"output_type":"stream","name":"stdout","text":["Working on 1. threats, plans to harm and incitement\n","Working on 2. derogation\n","Working on 3. animosity\n","Working on 4. prejudiced discussions\n"]},{"output_type":"stream","name":"stderr","text":["Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]},{"output_type":"stream","name":"stdout","text":["Working on 1. threats, plans to harm and incitement\n","Working on 2. derogation\n","Working on 3. animosity\n","Working on 4. prejudiced discussions\n"]},{"output_type":"stream","name":"stderr","text":["Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]},{"output_type":"stream","name":"stdout","text":["Working on 1. threats, plans to harm and incitement\n","Working on 2. derogation\n","Working on 3. animosity\n","Working on 4. prejudiced discussions\n","F1:0.271±0.019, SKIP: 0.115±0.016\n","P:0.287±0.023, R: 0.286±0.027\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"VAjHcEP1oXpC","executionInfo":{"status":"ok","timestamp":1675616613349,"user_tz":0,"elapsed":23,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MpZfR7daowME","executionInfo":{"status":"ok","timestamp":1675616613349,"user_tz":0,"elapsed":21,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WT_yeDLMowOp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = pd.read_csv(\"../Data/starting_ki/train_all_tasks.csv\")\n","train = train[train[target_column]!=\"none\"]\n","\n","lexicons = get_lexicons(train, shap_values)\n","newlexicons = merge_subwords(lexicons, train)\n","filtered_lexicons = filter_lexicons(newlexicons, q=0.90)  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q2oWfJecowRh","executionInfo":{"status":"ok","timestamp":1675616623906,"user_tz":0,"elapsed":10578,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}},"outputId":"5dc9f5fc-6d38-46f7-9464-0e67b07b6635"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stderr","text":["Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]},{"output_type":"stream","name":"stdout","text":["Working on 1. threats, plans to harm and incitement\n","Working on 2. derogation\n","Working on 3. animosity\n","Working on 4. prejudiced discussions\n"]}]},{"cell_type":"code","source":["for c in filtered_lexicons:\n","  filtered_lexicons[c] = {k: v for k, v in sorted(filtered_lexicons[c].items(), key=lambda item: -item[1])}\n","\n","  print(c, list(filtered_lexicons[c].keys())[0:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ZgnTo9powUd","executionInfo":{"status":"ok","timestamp":1675616623906,"user_tz":0,"elapsed":11,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}},"outputId":"24f55b4d-35bc-4634-e2d3-a8383c82d3bf"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["1. threats, plans to harm and incitement ['whores', 'skank', 'whore', 'pussies', 'bitches', 'bitch', 'pussy', 'feminism', 'ww3', 'uniteamerica']\n","2. derogation ['hillaryclintonisabitch', 'bhahahahahaahahahahaha', 'noodlewhore', 'noodlefoids', \"bitch's\", 'bitches', 'bitchy', 'pussies', 'fembots', 'bitch']\n","3. animosity ['muhahahahahahahahahahahahahahahaha', 'pussyfooting', 'bitchtard', 'bitchboi', 'bitches', 'autothots', 'bitching', 'bitchy', 'npcunt', 'bitch']\n","4. prejudiced discussions ['bitch', 'whorellywood', 'pussypass', 'feminazis', 'feminazi', 'pussy', 'whore', 'femoid', 'pussies', 'trumpaccusers']\n"]}]},{"cell_type":"code","source":["import json\n","with open('Results/TaskB/lexicon_shapley.json', 'w') as outfile:\n","    json.dump(filtered_lexicons, outfile)"],"metadata":{"id":"jzOht-tNpa55","executionInfo":{"status":"ok","timestamp":1675616624337,"user_tz":0,"elapsed":435,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NQlkT2WxowZ4"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}