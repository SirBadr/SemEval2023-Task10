{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17723,"status":"ok","timestamp":1675602082215,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"},"user_tz":0},"id":"sHyS00BLlWd6","outputId":"139fd8cf-5745-4ec7-febd-868a4b5ab82f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":889,"status":"ok","timestamp":1675602083100,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"},"user_tz":0},"id":"vd0eo8cegRT8","outputId":"ded7843f-902a-44e3-e9b3-a170c999d505"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/.shortcut-targets-by-id/1lC-ZKLaCDQyfLcof2Ak7FDa6IvTt318A/SemEval2023/SemEval2022-Task10/re_run\n"]}],"source":["cd /content/gdrive/MyDrive/SemEval2023/SemEval2022-Task10/re_run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mcg-u_-QghEO"},"outputs":[],"source":["import pandas as pd\n","# unsupervised_data_gab = pd.read_csv(\"Data/starting_ki/gab_1M_unlabelled.csv\")\n","# unsupervised_data_reddit = pd.read_csv(\"Data/starting_ki/reddit_1M_unlabelled.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DpYnTvGWRidT"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KxVljKIpjonw"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","target_column = \"label_vector\"\n","data = pd.read_csv(\"../Data/starting_ki/train_all_tasks.csv\")\n","data = data[data[target_column]!=\"none\"]\n","\n","# train, test = train_test_split(data, test_size=0.2, random_state=42)\n","# # data = train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MroRLHpw9xL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6Hq_qExqg3g"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KMhjlSKfsLbV"},"outputs":[],"source":["if target_column == \"label_category\":\n","  label_values = [\n","      '1. threats, plans to harm and incitement',\n","      '2. derogation',\n","      '3. animosity',\n","      '4. prejudiced discussions',\n","  ]\n","\n","elif target_column == \"label_vector\":\n","  label_values = [\n","      '1.1 threats of harm',\n","      '1.2 incitement and encouragement of harm',\n","      '2.1 descriptive attacks',\n","      '2.2 aggressive and emotive attacks',\n","      '2.3 dehumanising attacks & overt sexual objectification',\n","      '3.1 casual use of gendered slurs, profanities, and insults',\n","      '3.2 immutable gender differences and gender stereotypes',\n","      '3.3 backhanded gendered compliments',\n","      '3.4 condescending explanations or unwelcome advice',\n","      '4.1 supporting mistreatment of individual women',\n","      '4.2 supporting systemic discrimination against women as a group',\n","  ]"]},{"cell_type":"markdown","metadata":{"id":"S4LEXa0LTwbj"},"source":["## Shapley"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":19689,"status":"ok","timestamp":1675602110435,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"},"user_tz":0},"id":"hNKX-daoT0sc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5c976761-3e82-4e0c-9181-811a82d9a62c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.9/575.9 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# !pip install -q transformers shap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QiUYmNhwT4IQ"},"outputs":[],"source":["from transformers import BertTokenizer\n","swtokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SBrnlO3pUDgj"},"outputs":[],"source":["# import torch\n","# from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# out_dir = f'../Models/1a_fine-tuned-bert'\n","# model = BertForSequenceClassification.from_pretrained(out_dir)\n","\n","# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# model = model.to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bDGUbw-CTlYf"},"outputs":[],"source":["# import transformers\n","import torch\n","import numpy as np\n","import scipy as sp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6L0tWLQUZoH"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5v03chzpTlbH"},"outputs":[],"source":["import shap\n","# # define a prediction function\n","# def f(texts):\n","#   text_ids = [tokenizer.encode(text, max_length=100, padding='max_length', truncation=True) for text in texts]\n","\n","#   att_masks = []\n","#   for ids in text_ids:\n","#       masks = [int(id > 0) for id in ids]\n","#       att_masks.append(masks)\n","\n","#   text_ids = torch.tensor(text_ids).to(device)\n","#   att_masks = torch.tensor(att_masks).to(device)\n","\n","#   outputs = model(text_ids, attention_mask=att_masks)\n","#   outputs = outputs[0].detach().cpu().numpy()\n","#   scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n","  \n","#   val = sp.special.logit(scores[:,1]) # use one vs rest logit units\n","#   return val"]},{"cell_type":"markdown","metadata":{"id":"-esN0B13wPwr"},"source":["## Get Predictions from unannotated corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICCL8gO3IhtN"},"outputs":[],"source":["# texts = unsupervised_data_gab[\"text\"].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GruSizpEJoUt"},"outputs":[],"source":["# texts = texts[0:10000]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"46nLflDEJZ4L"},"outputs":[],"source":["# from tqdm import tqdm\n","# text_ids = []\n","# for text in tqdm(texts, total=len(texts)):\n","#   ids = tokenizer.encode(text, max_length=100, padding='max_length', truncation=True)\n","#   text_ids.append(ids)\n","\n","# text_ids_lengths = [len(text_ids[i]) for i in range(len(text_ids))]\n","# print(min(text_ids_lengths))\n","# print(max(text_ids_lengths))\n","\n","# att_masks = []\n","# for ids in text_ids:\n","#     masks = [int(id > 0) for id in ids]\n","#     att_masks.append(masks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"StGUFF6MKFSk"},"outputs":[],"source":["# text_ids = torch.tensor(text_ids);\n","# att_masks = torch.tensor(att_masks);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gAt2vmiSJKar"},"outputs":[],"source":["# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# batch_size = 32\n","\n","# test_data = TensorDataset(text_ids, att_masks)\n","# test_sampler = SequentialSampler(test_data)\n","# test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nYbA3zO3JKdH"},"outputs":[],"source":["# outputs = []\n","# with torch.no_grad():\n","#     model.eval()\n","#     for k, (mb_x, mb_m) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n","#         mb_x = mb_x.to(device)\n","#         mb_m = mb_m.to(device)\n","#         output = model(mb_x, attention_mask=mb_m)\n","#         outputs.append(output[0].to('cpu'))\n","\n","# outputs = torch.cat(outputs)\n","\n","# _, predicted_values = torch.max(outputs, 1)\n","# predicted_values = predicted_values.numpy()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPbPpVYIVllx"},"outputs":[],"source":["# predicted_labels = [\"sexist\" if p==1 else \"not sexist\" for p in predicted_values]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D0kqfuQFMk2B"},"outputs":[],"source":["# unsupervised_data_gab[\"pred\"] = predicted_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wj7lNdPoMk5U"},"outputs":[],"source":["# unsupervised_data_gab.to_csv(\"Results/predicted_data_gab.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_z1vOAtlLbpJ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"DitPuc2MFYr3"},"source":["## Get Shapley Values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7s6wqaaJKlZ"},"outputs":[],"source":["unsupervised_data_gab = pd.read_csv(\"../private_space/Results/predicted_data_gab.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j752cor-wr4L"},"outputs":[],"source":["unsupervised_data_gab = unsupervised_data_gab[unsupervised_data_gab[\"pred\"]==\"sexist\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1oZDhfaeYZt2"},"outputs":[],"source":["# import pickle\n","\n","# def save_shap_values(filepath, obj):\n","#   with open(filepath, 'wb') as fin:\n","#     pickle.dump(obj, fin)\n","\n","# def save_lexicons(filepath, obj):\n","#   with open(filepath, 'w', encoding=\"utf-8\") as fin:\n","#     for w in obj:\n","#       fin.write(str(w)+\"\\n\")\n","\n","# def get_lexicons(shap_values):\n","#   feature_names = shap_values.abs.mean(0).feature_names\n","#   shapley_values = shap_values.abs.mean(0).values\n","\n","#   candidates = [x for v, x in sorted(zip(shapley_values, feature_names), key=lambda pair: -pair[0]) if v > 0]\n","#   return candidates"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S6CsRhYcJ49S"},"outputs":[],"source":["# len(unsupervised_data_gab)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HmVucfCiyWJ6"},"outputs":[],"source":["# d = unsupervised_data_gab\n","# d = {\"text\": d[\"text\"].values}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"naGl8GmMl1pR"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5fV1DEgAv_ZG"},"outputs":[],"source":["# explainer = shap.Explainer(f, tokenizer)\n","# shap_values = explainer(d, fixed_context=1, batch_size=512)\n","# save_shap_values(f\"./Results/shapley_values_gab.pickle\", shap_values)\n","# lexicons = get_lexicons(shap_values)\n","# save_lexicons(f\"./Results/shapley_lexicon_gab.txt\", lexicons)\n","# print(\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Keh4mWgVxREG"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1BamihSDxSdp"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OWCxUZ9WxSgz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8OL9rhbxSjZ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nJYdfXH7xSmz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bjOTRECqxSp6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8655,"status":"ok","timestamp":1675602247427,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"},"user_tz":0},"id":"kYPak8UNYLf9","outputId":"07c566f4-b931-4575-cc64-09494e5cc963"},"outputs":[{"output_type":"stream","name":"stderr","text":["Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]}],"source":["import pickle\n","\n","def load_shap_values(filepath):\n","  with open(filepath, 'rb') as fin:\n","    obj = pickle.load(fin)\n","  return obj\n","\n","def get_lexicons(shap_values):\n","  feature_names = shap_values.abs.mean(0).feature_names\n","  shapley_values = shap_values.abs.mean(0).values\n","\n","  sorted_values = sorted(zip(shapley_values, feature_names), key=lambda pair: -pair[0])\n","  values = {x:v for v, x in sorted_values}\n","  return values\n","  \n","\n","shap_values = load_shap_values(f\"../Results/shapley_values_gab.pickle\")\n","lexicons_gab = get_lexicons(shap_values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QO8NIuafTlh4"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"jTxTc7NJyp_f"},"source":["# Handle subwords"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":619,"status":"ok","timestamp":1675602248254,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"},"user_tz":0},"id":"JLzzyV46yszN","outputId":"c5b65a53-964e-48fb-8570-3e7ea9b1f052"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":29}],"source":["import nltkz\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYFCwpHZzMhK"},"outputs":[],"source":["from nltk.tokenize import TweetTokenizer\n","nltktokenizer = TweetTokenizer()\n","\n","def word_tokenize(sent):\n","  # return nltk.word_tokenize(sent)\n","  return nltktokenizer.tokenize(sent)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h-Z_mTppLLeb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYRqFHXCyrSZ"},"outputs":[],"source":["from collections import defaultdict\n","\n","def map_subwords(words, subwords):\n","  sidx = 0\n","  widx = 0\n","  w = \"\"\n","  mapping = []\n","\n","  while widx < len(words):\n","    w = w + words[widx]\n","    if sidx >= len(subwords):\n","      print(widx, w, sidx)\n","      print(words)\n","      print(subwords)\n","\n","    idx = [sidx]\n","    s = subwords[sidx].replace(\"#\", \"\")\n","\n","    while len(w) < len(s):\n","      widx += 1\n","      w = w + words[widx].strip()\n","\n","    # if words[0] == \"judge\":\n","    #     print(w, s)\n","\n","    while s!=w and len(s) < len(w) and sidx+1 < len(subwords):\n","      sidx += 1\n","      s += subwords[sidx].replace(\"#\", \"\").strip()\n","      idx.append(sidx)\n","\n","      # if words[0] == \"judge\":\n","      #   print(f\"[{w}] [{s}]\", len(w), len(s), w==s)\n","\n","    if len(s) > len(w):\n","      widx += 1\n","      sidx -= len(idx) - 1\n","      continue\n","    \n","  \n","    if len(w) > 20 and w not in words:\n","      print(\"ERROR\", w)\n","      raise Exception((\"Strangly Long Word Found\"))\n","\n","    sidx += 1\n","    # print(w, s, idx)\n","    widx += 1\n","    w = \"\"\n","\n","    mapping.append(idx)\n","\n","  return mapping\n","\n","def normalise_quote(tokens):\n","  for widx, w in enumerate(tokens):\n","    if w==\"``\":\n","      tokens[widx] = '\"'\n","    elif w==\"''\":\n","      tokens[widx] = '\"'\n","    elif w==\"... ...\":\n","      tokens[widx] = '......'\n","      \n","  return tokens\n","\n","import re\n","\n","def clean_and_tokenize(sent):\n","  sent = sent.lower()\\\n","          .replace(\"#\", \"\")\\\n","          .replace(\"''\", '\"')\\\n","          .replace(\".\", \" \")\\\n","          .replace(\"-\", \" \")\\\n","          .replace(\"&x200b;\", \"\")\\\n","\n","  sent = re.sub('\\(\\d+\\) \\d\\d\\d \\d\\d\\d\\d', '[num]', sent)\n","\n","\n","  # remove more than 3 consecutive repeated characters\n","  groups = groupby(sent)\n","  sent = \"\".join([label*min(3, sum(1 for _ in group)) for label, group in groups])\n","  \n","\n","  sent = sent.encode(\"ascii\", \"ignore\")\n","  sent = sent.decode()\n","  \n","  words = word_tokenize(sent)\n","  words = normalise_quote(words)\n","    \n","  subwords = swtokenizer.tokenize(sent)\n","  subwords = normalise_quote(subwords)\n","  return words, subwords\n","\n","def merge_score(new_lexicons, lexicons, idxs, subwords):\n","  for idx in idxs:\n","    w = \"\"\n","    s = 0\n","    for i in idx:\n","      sw = subwords[i].replace(\"#\", \"\")\n","      w += sw\n","      if sw not in lexicons:\n","      #   print(\">>\", sw)\n","        continue\n","\n","      s += lexicons[sw]\n","      \n","    new_lexicons[w] = max(new_lexicons[w], s)\n","  return new_lexicons"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ls-fgFW7wUvb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B1cILFs0DvgU"},"outputs":[],"source":["# from tqdm import tqdm\n","# from itertools import groupby\n","\n","# new_lexicons = defaultdict(int)\n","# vocabs = set()\n","\n","# d = unsupervised_data_gab\n","# texts = d[\"text\"].values\n","\n","# for sent in tqdm(texts, total=len(texts)):\n","#   try:\n","#     words, subwords = clean_and_tokenize(sent)\n","#     idxs = map_subwords(words, subwords)\n","#     merge_score(new_lexicons, lexicons_gab, idxs, subwords)\n","#   except Exception as e:\n","#     print(\"Error\", str(e))\n","\n","# sorted_list = sorted(new_lexicons.items(), key=lambda item: -item[1])\n","# new_lexicons_gab = { k:v for k, v in sorted_list if v > 0}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iBdA5V6hZrcY"},"outputs":[],"source":["# import json\n","# with open(f'Results/lexicon_gab.json', 'w') as outfile:\n","#       json.dump(new_lexicons_gab, outfile)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"It74Sn0jyFFM"},"outputs":[],"source":["import json\n","with open(f'Results/lexicon_gab.json') as fin:\n","    new_lexicons_gab = json.load(fin)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yGNQLfA2yFHy"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53_ts2ejyFK6"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"DyTI-RqSyYQS"},"source":["## Determine their class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YLtRRlYCmNPI"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4wAIXsOyekD"},"outputs":[],"source":["with open(\"./Results/PMI_gab.pickle\", 'rb') as fin:\n","    PMI = pickle.load(fin)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jv_mRuvnmhmi"},"outputs":[],"source":["# Estimate Threshold\n","import random\n","import numpy as np\n","import math\n","\n","def get_PMI(PMI, wa, wb):\n","  PMI_counter, PMI_vocabs, PMI_Nx, PMI_Nxy = PMI\n","\n","  Px = PMI_counter[(None, wa)]/PMI_Nx\n","  Py = PMI_counter[(None, wb)]/PMI_Nx\n","\n","  if Px==0 or Py==0:\n","    return 0\n","    \n","  Pxy = PMI_counter[(wa, wb)]/PMI_Nxy\n","\n","  delta = 1e-10\n","  p = max(math.log2((Pxy+delta)/(Px*Py)), 0)\n","  return p\n","\n","# N = int(len(PMI[1])*0.5)\n","N = 100000\n","wordA = random.sample(PMI[1], N)\n","wordB = random.sample(PMI[1], N)\n","values = [get_PMI(PMI, wa, wb) for wa, wb in zip(wordA, wordB)]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":317,"status":"ok","timestamp":1675602326190,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"},"user_tz":0},"id":"nYGDF-lCyfqH","outputId":"69c59ea4-7635-4eb5-be49-0aaa700a3a13"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["13.35977390501769"]},"metadata":{},"execution_count":37}],"source":["threshold = np.quantile(values, 0.99)\n","threshold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15TBFu1FzE9r"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MOjgH-kEzBBo"},"outputs":[],"source":["def augment(PMI, vocabs, candidates, lexicons, sqrt=False):\n","  augmented_lexicons = {}\n","  PMI_counter, PMI_vocabs, PMI_Nx, PMI_Nxy = PMI\n","\n","  for v in tqdm(vocabs, total=len(vocabs)):\n","    if v not in candidates:\n","      augmented_lexicons[v] = []\n","      continue\n","\n","    selected = []\n","    for k in candidates:\n","      pmi = get_PMI(PMI, v, k)\n","      if pmi > threshold:\n","        selected.append((k, candidates[k]))\n","    augmented_lexicons[v] = selected\n","    \n","  new_lexicons = {}\n","  for label in lexicons:\n","    new_lexicon = {}\n","    for word in lexicons[label]:\n","      for new_word, shap in augmented_lexicons[word]:\n","        if sqrt:\n","          val = math.sqrt(lexicons[label][word]*shap)\n","        else:\n","          val = shap \n","        \n","        if new_word in new_lexicon:\n","          new_lexicon[new_word] = min(val, new_lexicon[new_word])\n","        else:\n","          new_lexicon[new_word] = val\n","\n","    new_lexicons[label] = {}\n","    for word in lexicons[label]:\n","      new_lexicons[label][word] = lexicons[label][word]\n","\n","    for word in new_lexicon:\n","      new_lexicons[label][word] = new_lexicon[word]\n","    \n","  return new_lexicons"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DiX2IL18SZ0x"},"outputs":[],"source":["# import json\n","# with open(f'Results/TaskB/lexicon_shapley_train_0.json') as fin:\n","#   lexicon_shapley = json.load(fin)\n","\n","# vocabs = set()\n","# for label in lexicon_shapley:\n","#   vocabs.update(lexicon_shapley[label].keys())\n","\n","# augmented_lexicons = augment(PMI, vocabs, new_lexicons_gab, lexicon_shapley)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eNJsf6-LzfJk"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ah0BPNGPnEh8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pjzPua720-vW"},"outputs":[],"source":[]},{"cell_type":"code","source":["# for k in augmented_lexicons:\n","#   print(k, len(lexicon_shapley[k]), len(augmented_lexicons[k]))"],"metadata":{"id":"pfm3_QNpvYzj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bRK2b1GpTMMz"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4rKca_AJTNol"},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","def predict(word, lexicons):\n","  cnt = {}\n","  for label in label_values:\n","    cnt[label] = 0\n","\n","  for w in word:\n","    for label in label_values:\n","      if w in lexicons[label]:\n","        cnt[label] += lexicons[label][w]\n","\n","  \n","  if sum(cnt.values())==0:\n","    return None\n","  \n","  return max(cnt.items(), key=lambda k: k[1])[0]\n","\n","def run_predict(test_words, test_labels, lexicons, return_predict=False):\n","  y_pred = []\n","  y_test = []\n","  non = 0\n","  for word, label in zip(test_words, test_labels):\n","    pred = predict(word, lexicons)\n","    if pred is None:\n","      non += 1\n","      continue\n","\n","    y_pred.append(pred)\n","    y_test.append(label)\n","\n","  f1 = f1_score(y_test, y_pred, average='macro')\n","  skip = non/len(test_words)\n","\n","  if return_predict:\n","    return f1, skip, y_test, y_pred\n","    \n","  # print(\"F1:\", f1)\n","  # print(\"Skip:\", skip)\n","  return f1, skip\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1675602326194,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"},"user_tz":0},"id":"E3nYPYQuUkHi","outputId":"ebdb1817-3a59-47f8-8b8d-5143565d6410"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'label_vector'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":42}],"source":["target_column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OhAK7sQ1TN7r"},"outputs":[],"source":["# val = pd.read_csv(f\"Data/0_val.csv\")\n","# texts = val[\"text\"].values\n","# val_words = [word_tokenize(s) for s in texts]\n","# val_labels = val[target_column].values\n","\n","# f1, skip = run_predict(val_words, val_labels, augmented_lexicons)\n","# print(\"F1:\", f1)\n","# print(\"Skip:\", skip)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YNiv48CjTZMP"},"outputs":[],"source":["# f1, skip = run_predict(val_words, val_labels, lexicon_shapley)\n","# print(\"F1:\", f1)\n","# print(\"Skip:\", skip)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rPOGtT49TZTD"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"6bHQ8XF7TdbD"},"source":["## Evaluate Lexicons"]},{"cell_type":"code","source":["ls -al Results/TaskC/lexicon_pmi_train_*"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s1bvenGJ10YD","executionInfo":{"status":"ok","timestamp":1675605634041,"user_tz":0,"elapsed":317,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}},"outputId":"53454216-1bfb-42cd-d1a6-ba9c0dd55150"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-rw------- 1 root root 262473 Feb  5 13:59 Results/TaskC/lexicon_pmi_train_0.json\n","-rw------- 1 root root 263387 Feb  5 13:59 Results/TaskC/lexicon_pmi_train_1.json\n","-rw------- 1 root root 262409 Feb  5 13:59 Results/TaskC/lexicon_pmi_train_2.json\n","-rw------- 1 root root 261371 Feb  5 13:59 Results/TaskC/lexicon_pmi_train_3.json\n","-rw------- 1 root root 258938 Feb  5 13:59 Results/TaskC/lexicon_pmi_train_4.json\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oP2ULpmXTgH5","outputId":"8ca58a82-63d9-4487-ead9-109686f9ddd8","executionInfo":{"status":"ok","timestamp":1675608021407,"user_tz":0,"elapsed":2382776,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 7358/7358 [08:09<00:00, 15.04it/s]\n","100%|██████████| 7372/7372 [07:37<00:00, 16.11it/s]\n","100%|██████████| 7383/7383 [08:13<00:00, 14.96it/s]\n","100%|██████████| 7340/7340 [07:45<00:00, 15.76it/s]\n","100%|██████████| 7337/7337 [07:51<00:00, 15.57it/s]\n"]}],"source":["import json\n","from sklearn.metrics import precision_recall_fscore_support\n","from tqdm import tqdm\n","\n","all_f1 = []\n","all_skip = []\n","all_p = []\n","all_r = []\n","\n","aug_all_f1 = []\n","aug_all_skip = []\n","aug_all_p = []\n","aug_all_r = []\n","\n","aug = \"pmi\"\n","for i in range(5):\n","  with open(f'Results/TaskC/lexicon_{aug}_train_{i}.json') as fin:\n","    lexicon_shapley = json.load(fin)\n","\n","  vocabs = set()\n","  for label in lexicon_shapley:\n","    vocabs.update(lexicon_shapley[label].keys())\n","\n","  augmented_lexicons = augment(PMI, vocabs, new_lexicons_gab, lexicon_shapley, sqrt=True)\n","  with open(f'Results/TaskC/augmented_lexicon_{aug}_train_{i}.json', 'w') as outfile:\n","      json.dump(augmented_lexicons, outfile)\n","\n","  val = pd.read_csv(f\"Data/{i}_test.csv\")\n","  texts = val[\"text\"].values\n","  val_words = [word_tokenize(s) for s in texts]\n","  val_labels = val[target_column].values\n","\n","  f1, skip, y_test, y_pred = run_predict(val_words, val_labels, lexicon_shapley, return_predict=True)\n","  p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n","\n","  all_p.append(p)\n","  all_r.append(r)\n","  all_f1.append(f1)\n","  all_skip.append(skip)\n","\n","  aug_f1, aug_skip, y_test, y_pred = run_predict(val_words, val_labels, augmented_lexicons, return_predict=True)\n","  p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n","  aug_all_p.append(p)\n","  aug_all_r.append(r)\n","  aug_all_f1.append(aug_f1)\n","  aug_all_skip.append(aug_skip)"]},{"cell_type":"code","source":[],"metadata":{"id":"Juf6Xpc22x3r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qPRPfRLX2x7I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zQtRHCMX2yAh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Lexicon PMI"],"metadata":{"id":"f9Ce2IYz2zwx"}},{"cell_type":"code","source":["print(f\"F1:{np.mean(all_f1):.3f} >> {np.mean(aug_all_f1):.3f}\")\n","print(f\"SKIP:{np.mean(all_skip):.3f} >> {np.mean(aug_all_skip):.3f}\")\n","print(f\"P:{np.mean(all_p):.3f} >> {np.mean(aug_all_p):.3f}\")\n","print(f\"R:{np.mean(all_r):.3f} >> {np.mean(aug_all_r):.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DbhH3A0R2yEf","executionInfo":{"status":"ok","timestamp":1675608021409,"user_tz":0,"elapsed":18,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}},"outputId":"a428d1c5-b7e7-4100-cdfb-3d546bc05ea1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["F1:0.173 >> 0.169\n","SKIP:0.088 >> 0.079\n","P:0.189 >> 0.186\n","R:0.220 >> 0.207\n"]}]},{"cell_type":"code","source":["print(f\"F1:{np.mean(aug_all_f1):.3f} ± {np.std(aug_all_f1):.3f}\")\n","print(f\"SKIP:{np.mean(aug_all_skip):.3f} ± {np.std(aug_all_skip):.3f}\")\n","print(f\"P:{np.mean(aug_all_p):.3f} ± {np.std(aug_all_p):.3f}\")\n","print(f\"R:{np.mean(aug_all_r):.3f} ± {np.std(aug_all_r):.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qp1D5Sx72uoL","executionInfo":{"status":"ok","timestamp":1675608021409,"user_tz":0,"elapsed":12,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}},"outputId":"7e73b9de-7ba9-40e4-b35f-e6be9f1a5dd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["F1:0.169 ± 0.012\n","SKIP:0.079 ± 0.014\n","P:0.186 ± 0.017\n","R:0.207 ± 0.017\n"]}]},{"cell_type":"code","source":["aug"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"TNfWWeyb25Ne","executionInfo":{"status":"ok","timestamp":1675608021409,"user_tz":0,"elapsed":10,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}},"outputId":"3f9737f2-3b1a-46f0-d5dd-59dc37d646ae"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'pmi'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":69}]},{"cell_type":"code","source":[],"metadata":{"id":"Dwd8mzga25Ry"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Lexicon Shapley"],"metadata":{"id":"hYH86YmD2vB7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQg-LzRzlrZB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675604388020,"user_tz":0,"elapsed":27,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}},"outputId":"05076989-5198-4f61-d20d-cf0b5e5bdc24"},"outputs":[{"output_type":"stream","name":"stdout","text":["F1:0.139 >> 0.142\n","SKIP:0.000 >> 0.000\n","P:0.179 >> 0.178\n","R:0.161 >> 0.163\n"]}],"source":["print(f\"F1:{np.mean(all_f1):.3f} >> {np.mean(aug_all_f1):.3f}\")\n","print(f\"SKIP:{np.mean(all_skip):.3f} >> {np.mean(aug_all_skip):.3f}\")\n","print(f\"P:{np.mean(all_p):.3f} >> {np.mean(aug_all_p):.3f}\")\n","print(f\"R:{np.mean(all_r):.3f} >> {np.mean(aug_all_r):.3f}\")"]},{"cell_type":"code","source":["print(f\"F1:{np.mean(aug_all_f1):.3f} ± {np.std(aug_all_f1):.3f}\")\n","print(f\"SKIP:{np.mean(aug_all_skip):.3f} ± {np.std(aug_all_skip):.3f}\")\n","print(f\"P:{np.mean(aug_all_p):.3f} ± {np.std(aug_all_p):.3f}\")\n","print(f\"R:{np.mean(aug_all_r):.3f} ± {np.std(aug_all_r):.3f}\")"],"metadata":{"id":"w6XJTTAxrGSh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675604388020,"user_tz":0,"elapsed":8,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}},"outputId":"6c093f4a-4449-45d5-8a80-c206e20a23ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["F1:0.142 ± 0.018\n","SKIP:0.000 ± 0.000\n","P:0.178 ± 0.018\n","R:0.163 ± 0.037\n"]}]},{"cell_type":"code","source":["# list(zip(all_f1, aug_all_f1))"],"metadata":{"id":"zJJ6YqPmrYuE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"a3oQD0TOrYyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IGKEKlofrY0b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(f'Results/TaskC/lexicon_{aug}.json') as fin:\n","  lexicon_shapley = json.load(fin)\n","\n","vocabs = set()\n","for label in lexicon_shapley:\n","  vocabs.update(lexicon_shapley[label].keys())\n","\n","augmented_lexicons = augment(PMI, vocabs, new_lexicons_gab, lexicon_shapley)\n","with open(f'Results/TaskC/augmented_lexicon_{aug}.json', 'w') as outfile:\n","    json.dump(augmented_lexicons, outfile)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oOY8NBelrY3H","executionInfo":{"status":"ok","timestamp":1675608550910,"user_tz":0,"elapsed":529509,"user":{"displayName":"Pakawat Nakwijit","userId":"01430675152978466058"}},"outputId":"317d84e5-ff52-42c8-b881-2c7c1b397c0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 8382/8382 [08:49<00:00, 15.84it/s]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"xY39LiFurUJF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8DdQRylx4Hil"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"a0IBoAIw4Hlu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kQA5kDIi4Hoi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dOvALXZ24HrY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zwXpEsMn4HuB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KK64k8bG4Hw5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iQL-gFpa4dl6"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}